{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceTextGenInference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_config = {\n",
    "    \"inference_server_url\":\"http://10.10.78.11:8081/\",\n",
    "    \"max_new_tokens\":512,\n",
    "    \"top_k\":10,\n",
    "    \"top_p\":0.95,\n",
    "    \"typical_p\":0.95,\n",
    "    \"temperature\":0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_chain(simpler_text:str, original_text:str, prompt_text:str, input_variables:list)-> str:\n",
    "    \"\"\"Executes a chain for a given task\"\"\"\n",
    "    llm = HuggingFaceTextGenInference(**mixtral_config)\n",
    "    # set prompt\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=prompt_text,\n",
    "        input_variables=input_variables,\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    llm_response = chain({'text':simpler_text})[\"text\"].strip()\n",
    "        \n",
    "    return llm_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = execute_chain(simpler_text, original_text, prompt_text, input_variables)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
